"""
éŸ³å£°æ„Ÿæƒ…èªè­˜ï¼ˆSpeech Emotion Recognitionï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

æ—¥æœ¬èªéŸ³å£°ã«å¯¾å¿œã—ãŸXLS-Rãƒ™ãƒ¼ã‚¹ã®å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€
éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ„Ÿæƒ…ã‚’èªè­˜ã—ã¾ã™ã€‚
"""
import time
from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import soundfile as sf
import torch
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor


class EmotionRecognizer:
    """éŸ³å£°æ„Ÿæƒ…èªè­˜ã‚¯ãƒ©ã‚¹"""

    # æ—¥æœ¬èªéŸ³å£°æ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«ï¼ˆBagus/wav2vec2-xlsr-japanese-speech-emotion-recognitionï¼‰
    DEFAULT_MODEL = "Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition"
    
    # JTESãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«
    EMOTION_LABELS = {
        0: "angry",    # æ€’ã‚Š
        1: "happy",    # å–œã³
        2: "neutral",  # ä¸­ç«‹
        3: "sad",      # æ‚²ã—ã¿
    }

    def __init__(
        self,
        model_name: Optional[str] = None,
        device: Optional[str] = None,
        verbose: bool = True
    ):
        """
        åˆæœŸåŒ–

        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆçœç•¥æ™‚ã¯DEFAULT_MODELã‚’ä½¿ç”¨ï¼‰
            device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ï¼ˆ'cuda', 'cpu', Noneã§è‡ªå‹•æ¤œå‡ºï¼‰
            verbose: å‡¦ç†çŠ¶æ³ã‚’å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.verbose = verbose
        self.model_name = model_name or self.DEFAULT_MODEL
        
        # ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®šï¼ˆGPUè‡ªå‹•æ¤œå‡ºï¼‰
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        self._print(f"ğŸ”§ éŸ³å£°æ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
        self._print(f"   ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        self._print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        if self.device == "cuda":
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            self._print(f"   GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # ãƒ¢ãƒ‡ãƒ«ã¨feature extractorã®èª­ã¿è¾¼ã¿
        try:
            self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
                self.model_name
            )
            self.model = Wav2Vec2ForSequenceClassification.from_pretrained(
                self.model_name
            )
            self.model.to(self.device)
            self.model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
            
            self._print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")
            
        except Exception as e:
            self._print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            raise

    def _print(self, message: str) -> None:
        """verboseãŒTrueã®å ´åˆã®ã¿å‡ºåŠ›"""
        if self.verbose:
            print(message)

    def _load_and_preprocess_audio(
        self, 
        audio_file: str
    ) -> Tuple[torch.Tensor, int]:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œ

        Args:
            audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            (waveform, sample_rate)ã®ã‚¿ãƒ—ãƒ«
        """
        # soundfileã§éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆtorchaudioäº’æ›æ€§å•é¡Œã‚’å›é¿ï¼‰
        waveform, sample_rate = sf.read(audio_file, dtype='float32')
        
        # numpyé…åˆ—ã‚’tensorã«å¤‰æ›
        waveform = torch.from_numpy(waveform)
        
        # ãƒ¢ãƒãƒ©ãƒ«ã§ãªã„å ´åˆã¯å¤‰æ›
        if waveform.ndim == 1:
            # ãƒ¢ãƒãƒ©ãƒ«: (samples,) -> (1, samples)
            waveform = waveform.unsqueeze(0)
        elif waveform.ndim == 2:
            # ã‚¹ãƒ†ãƒ¬ã‚ª: (samples, channels) -> (channels, samples)
            waveform = waveform.T
            # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # 16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¿…è¦ãªå ´åˆï¼‰
        if sample_rate != 16000:
            # scipy.signalã§ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            from scipy import signal
            num_samples = int(waveform.shape[1] * 16000 / sample_rate)
            waveform_np = waveform.squeeze().numpy()
            waveform_resampled = signal.resample(waveform_np, num_samples)
            waveform = torch.from_numpy(waveform_resampled).unsqueeze(0).float()
            sample_rate = 16000
        
        return waveform, sample_rate

    def recognize_emotion(
        self, 
        audio_file: str
    ) -> Dict[str, any]:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ„Ÿæƒ…ã‚’èªè­˜

        Args:
            audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            æ„Ÿæƒ…èªè­˜çµæœã®è¾æ›¸
            {
                'emotions': {'angry': 0.1, 'happy': 0.3, 'neutral': 0.5, 'sad': 0.1},
                'dominant_emotion': 'neutral',
                'confidence': 0.5,
                'processing_time': 0.123,
                'error': None
            }
        """
        start_time = time.time()
        
        try:
            # éŸ³å£°ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
            waveform, sample_rate = self._load_and_preprocess_audio(audio_file)
            
            # numpyé…åˆ—ã«å¤‰æ›ï¼ˆfeature extractorã®å…¥åŠ›å½¢å¼ï¼‰
            audio_array = waveform.squeeze().numpy()
            
            # Feature extraction
            inputs = self.feature_extractor(
                audio_array,
                sampling_rate=sample_rate,
                return_tensors="pt",
                padding=True
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            inputs = {key: value.to(self.device) for key, value in inputs.items()}
            
            # æ¨è«–å®Ÿè¡Œ
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
            
            # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§ç¢ºç‡ã«å¤‰æ›
            probabilities = torch.nn.functional.softmax(logits, dim=-1)
            probabilities = probabilities.cpu().numpy()[0]
            
            # æ„Ÿæƒ…ã”ã¨ã®ã‚¹ã‚³ã‚¢
            emotions = {
                self.EMOTION_LABELS[i]: float(probabilities[i])
                for i in range(len(probabilities))
            }
            
            # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„æ„Ÿæƒ…
            dominant_idx = np.argmax(probabilities)
            dominant_emotion = self.EMOTION_LABELS[dominant_idx]
            confidence = float(probabilities[dominant_idx])
            
            processing_time = time.time() - start_time
            
            return {
                'emotions': emotions,
                'dominant_emotion': dominant_emotion,
                'confidence': confidence,
                'processing_time': processing_time,
                'error': None
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            self._print(f"âŒ æ„Ÿæƒ…èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
            
            return {
                'emotions': {},
                'dominant_emotion': None,
                'confidence': 0.0,
                'processing_time': processing_time,
                'error': str(e)
            }

    def recognize_emotion_batch(
        self, 
        audio_files: list[str],
        show_progress: bool = True
    ) -> list[Dict[str, any]]:
        """
        è¤‡æ•°ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ„Ÿæƒ…ã‚’ä¸€æ‹¬èªè­˜

        Args:
            audio_files: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            show_progress: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            æ„Ÿæƒ…èªè­˜çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []
        
        if show_progress:
            try:
                from tqdm import tqdm
                iterator = tqdm(audio_files, desc="æ„Ÿæƒ…èªè­˜å‡¦ç†ä¸­")
            except ImportError:
                self._print("âš ï¸  tqdmãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãªã—ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
                iterator = audio_files
        else:
            iterator = audio_files
        
        for audio_file in iterator:
            result = self.recognize_emotion(audio_file)
            result['filename'] = Path(audio_file).name
            result['filepath'] = str(audio_file)
            results.append(result)
        
        return results

    def get_device_info(self) -> Dict[str, str]:
        """
        ä½¿ç”¨ä¸­ã®ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—

        Returns:
            ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®è¾æ›¸
        """
        info = {
            'device': self.device,
            'cuda_available': torch.cuda.is_available(),
        }
        
        if torch.cuda.is_available():
            info['gpu_name'] = torch.cuda.get_device_name(0)
            info['gpu_memory_total'] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB"
            info['gpu_memory_allocated'] = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f}GB"
        
        return info


def main():
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python emotion_recognizer.py <audio_file>")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    
    if not Path(audio_file).exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_file}")
        sys.exit(1)
    
    # æ„Ÿæƒ…èªè­˜ã®å®Ÿè¡Œ
    recognizer = EmotionRecognizer(verbose=True)
    
    print(f"\n{'=' * 80}")
    print(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {audio_file}")
    print(f"{'=' * 80}\n")
    
    result = recognizer.recognize_emotion(audio_file)
    
    if result['error'] is None:
        print(f"\nâœ… æ„Ÿæƒ…èªè­˜çµæœ:")
        print(f"   æ”¯é…çš„æ„Ÿæƒ…: {result['dominant_emotion']} (ä¿¡é ¼åº¦: {result['confidence']:.2%})")
        print(f"\n   æ„Ÿæƒ…ã‚¹ã‚³ã‚¢:")
        for emotion, score in sorted(result['emotions'].items(), key=lambda x: x[1], reverse=True):
            print(f"     {emotion:10s}: {score:.2%} {'â–ˆ' * int(score * 50)}")
        print(f"\n   å‡¦ç†æ™‚é–“: {result['processing_time']:.3f}ç§’")
    else:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
    
    # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®è¡¨ç¤º
    print(f"\n{'=' * 80}")
    print("ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±:")
    print(f"{'=' * 80}")
    device_info = recognizer.get_device_info()
    for key, value in device_info.items():
        print(f"  {key}: {value}")


if __name__ == "__main__":
    main()
